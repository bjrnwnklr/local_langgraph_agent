# local_langgraph_agent (Pythonâ€¯3.12Â â€¢ Windowsâ€¯11Â WSL2)

This repository bootstraps an **offline LangChainÂ +â€¯LangGraph playground** that runs **entirely inside a WSL2 distro** on WindowsÂ 11. The agent uses **PythonÂ 3.12**, **Ollama**, and **Chroma**â€”so no cloud keys are required.

> **Goal**Â â€“ go from an empty folder to a runnable agent you can chat with, plus an optional dev UI (LangGraphÂ Studio).

---

## 0Â Â·Â Prerequisites

| Tool   | MinimumÂ version | Notes                                                                               |
| ------ | --------------- | ----------------------------------------------------------------------------------- |
| Python | **3.12.x**      | Use the Microsoft Store or [`pyenv`](https://github.com/pyenv/pyenv) _inside_ WSL2. |
| Git    | any             | version control                                                                     |
| Docker | latest          | only for `langgraph dev` hotâ€‘reload server                                          |
| Ollama | 0.1.32Â or newer | oneâ€‘line install inside WSL2 (see stepâ€¯3)                                           |

> â„¹ï¸Â **Why Docker?**Â `langgraph dev` spins up its own container for liveâ€‘reload isolation. You can skip Docker entirely and just run `python main.py`.

---

## 1Â Â·Â ProjectÂ setup

```bash
# create & enter project folder (still inside WSL2)
mkdir local_langgraph_agent && cd local_langgraph_agent

# initialise Git (optional but recommended)
git init

# create PythonÂ 3.12 virtual environment
python3.12 -m venv .venv

# activate the venv (Linux-style path works in WSL)
source .venv/bin/activate
```

---

## 2Â Â·Â Install dependencies

```bash
# upgrade pip
pip install -U pip

# install everything declared in requirements.txt
pip install -r requirements.txt
```

> ðŸ”Â No API keys are needed; everything runs locally.

---

## 3Â Â·Â Install and start Ollama

```bash
# oneâ€‘liner inside WSL2 DebianÂ 12 (requires curl)
curl -fsSL https://ollama.com/install.sh | sh

# pull a 3â€“7â€¯GB model (e.g. LlamaÂ 3)
ollama pull llama3

# start the daemon in the background (listens on 127.0.0.1:11434)
ollama serve &
```

_The port is bound to localhost inside WSL2; your Windows host can still reach it at **`http://localhost:11434`** thanks to the automatic port proxy._

---

## 4Â Â·Â ProjectÂ layout

```
local_langgraph_agent/
â”œâ”€ main.py
â”œâ”€ langgraph.json
â”œâ”€ requirements.txt
â””â”€ .env.example
```

### `langgraph.json`

```json
{
    "dependencies": ["./"],
    "graphs": {
        "research_agent": "./main.py:build_agent"
    },
    "python_version": "3.12"
}
```

> `langgraph dev` reads this file to locate your graph factory and to spin up the dev server with the correct interpreter.

---

## 5Â Â·Â Run the agent

### Quick CLI test

```bash
python main.py
```

### Hotâ€‘reload dev serverÂ +Â LangGraphÂ Studio UI

```bash
langgraph dev
```

_Open _[**_http://localhost:2024_**](http://localhost:2024)_ in your Windows browser to inspect each node, timeâ€‘travel through state, and tweak code without restarting._

---

## 6Â Â·Â NextÂ steps

| Upgrade path           | How                                                                                   |
| ---------------------- | ------------------------------------------------------------------------------------- |
| **Add RAG**            | Ingest docs into Chroma and register a retriever tool.                                |
| **Multiâ€‘agent graphs** | Replace `create_react_agent` with a handâ€‘rolled `StateGraph`.                         |
| **Package for prod**   | `langgraph build` emits a Docker image; you can run it on any host or publish to ACR. |

---

Happy hackingÂ ðŸš€
